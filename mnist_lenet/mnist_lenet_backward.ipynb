{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ad80d14db838>:69: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Users\\lele\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Users\\lele\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\lele\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\lele\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Users\\lele\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "pool_shape[0]: 16\n",
      "After 1 steps,the loss is 5.175639\n",
      "After 101 steps,the loss is 2.093199\n",
      "After 201 steps,the loss is 1.187238\n",
      "After 301 steps,the loss is 0.669913\n",
      "After 401 steps,the loss is 0.847421\n",
      "After 501 steps,the loss is 0.747313\n",
      "After 601 steps,the loss is 0.680314\n",
      "After 701 steps,the loss is 0.667357\n",
      "After 801 steps,the loss is 1.055429\n",
      "After 901 steps,the loss is 0.640755\n",
      "After 1001 steps,the loss is 0.673610\n",
      "After 1101 steps,the loss is 0.676126\n",
      "After 1201 steps,the loss is 0.718006\n",
      "After 1301 steps,the loss is 1.066090\n",
      "After 1401 steps,the loss is 0.727668\n",
      "After 1501 steps,the loss is 0.629173\n",
      "After 1601 steps,the loss is 0.752331\n",
      "After 1701 steps,the loss is 0.858797\n",
      "After 1801 steps,the loss is 0.644868\n",
      "After 1901 steps,the loss is 0.743451\n",
      "After 2001 steps,the loss is 0.659485\n",
      "After 2101 steps,the loss is 0.623510\n",
      "After 2201 steps,the loss is 0.649839\n",
      "After 2301 steps,the loss is 0.703313\n",
      "After 2401 steps,the loss is 0.630919\n",
      "After 2501 steps,the loss is 1.053113\n",
      "After 2601 steps,the loss is 0.619624\n",
      "After 2701 steps,the loss is 0.626574\n",
      "After 2801 steps,the loss is 0.657032\n",
      "After 2901 steps,the loss is 0.614425\n",
      "After 3001 steps,the loss is 0.978504\n",
      "After 3101 steps,the loss is 0.625567\n",
      "After 3201 steps,the loss is 0.618891\n",
      "After 3301 steps,the loss is 0.608601\n",
      "After 3401 steps,the loss is 0.620591\n",
      "After 3501 steps,the loss is 0.700604\n",
      "After 3601 steps,the loss is 0.766301\n",
      "After 3701 steps,the loss is 0.684956\n",
      "After 3801 steps,the loss is 0.909017\n",
      "After 3901 steps,the loss is 0.610638\n",
      "After 4001 steps,the loss is 0.601208\n",
      "After 4101 steps,the loss is 0.601578\n",
      "After 4201 steps,the loss is 0.597598\n",
      "After 4301 steps,the loss is 0.612275\n",
      "After 4401 steps,the loss is 0.627701\n",
      "After 4501 steps,the loss is 0.594224\n",
      "After 4601 steps,the loss is 0.844018\n",
      "After 4701 steps,the loss is 0.610324\n",
      "After 4801 steps,the loss is 0.717005\n",
      "After 4901 steps,the loss is 0.590796\n",
      "After 5001 steps,the loss is 0.593007\n",
      "After 5101 steps,the loss is 0.599946\n",
      "After 5201 steps,the loss is 0.643231\n",
      "After 5301 steps,the loss is 0.617860\n",
      "After 5401 steps,the loss is 0.591883\n",
      "After 5501 steps,the loss is 0.589774\n",
      "After 5601 steps,the loss is 0.588353\n",
      "After 5701 steps,the loss is 0.588788\n",
      "After 5801 steps,the loss is 0.582323\n",
      "After 5901 steps,the loss is 0.723198\n",
      "After 6001 steps,the loss is 0.606674\n",
      "After 6101 steps,the loss is 0.682886\n",
      "After 6201 steps,the loss is 0.580085\n",
      "After 6301 steps,the loss is 0.600956\n",
      "After 6401 steps,the loss is 0.576601\n",
      "After 6501 steps,the loss is 0.573657\n",
      "After 6601 steps,the loss is 0.827755\n",
      "After 6701 steps,the loss is 1.303749\n",
      "After 6801 steps,the loss is 0.571762\n",
      "After 6901 steps,the loss is 0.610755\n",
      "After 7001 steps,the loss is 0.640113\n",
      "After 7101 steps,the loss is 0.569128\n",
      "After 7201 steps,the loss is 0.566080\n",
      "After 7301 steps,the loss is 0.564210\n",
      "After 7401 steps,the loss is 0.563264\n",
      "After 7501 steps,the loss is 0.585070\n",
      "After 7601 steps,the loss is 0.561940\n",
      "After 7701 steps,the loss is 0.560033\n",
      "After 7801 steps,the loss is 0.562185\n",
      "After 7901 steps,the loss is 0.558051\n",
      "After 8001 steps,the loss is 0.556984\n",
      "After 8101 steps,the loss is 0.556139\n",
      "After 8201 steps,the loss is 0.577099\n",
      "After 8301 steps,the loss is 0.554054\n",
      "After 8401 steps,the loss is 0.555307\n",
      "After 8501 steps,the loss is 0.551864\n",
      "After 8601 steps,the loss is 0.551032\n",
      "After 8701 steps,the loss is 0.553207\n",
      "After 8801 steps,the loss is 0.578730\n",
      "After 8901 steps,the loss is 0.733932\n",
      "After 9001 steps,the loss is 0.548291\n",
      "After 9101 steps,the loss is 0.547286\n",
      "After 9201 steps,the loss is 0.555260\n",
      "After 9301 steps,the loss is 0.544677\n",
      "After 9401 steps,the loss is 0.546517\n",
      "After 9501 steps,the loss is 0.554930\n",
      "After 9601 steps,the loss is 0.584289\n",
      "After 9701 steps,the loss is 0.554965\n",
      "After 9801 steps,the loss is 0.553989\n",
      "After 9901 steps,the loss is 0.540608\n",
      "After 10001 steps,the loss is 0.554199\n",
      "After 10101 steps,the loss is 0.535904\n",
      "After 10201 steps,the loss is 0.534765\n",
      "After 10301 steps,the loss is 0.566554\n",
      "After 10401 steps,the loss is 0.533440\n",
      "After 10501 steps,the loss is 0.538789\n",
      "After 10601 steps,the loss is 0.531668\n",
      "After 10701 steps,the loss is 0.530395\n",
      "After 10801 steps,the loss is 0.529498\n",
      "After 10901 steps,the loss is 0.527991\n",
      "After 11001 steps,the loss is 0.528321\n",
      "After 11101 steps,the loss is 0.526199\n",
      "After 11201 steps,the loss is 0.628311\n",
      "After 11301 steps,the loss is 0.526717\n",
      "After 11401 steps,the loss is 0.588199\n",
      "After 11501 steps,the loss is 0.523339\n",
      "After 11601 steps,the loss is 0.521923\n",
      "After 11701 steps,the loss is 0.520764\n",
      "After 11801 steps,the loss is 0.534810\n",
      "After 11901 steps,the loss is 0.530664\n",
      "After 12001 steps,the loss is 0.526832\n",
      "After 12101 steps,the loss is 0.519265\n",
      "After 12201 steps,the loss is 0.534826\n",
      "After 12301 steps,the loss is 0.525652\n",
      "After 12401 steps,the loss is 0.516336\n",
      "After 12501 steps,the loss is 0.549207\n",
      "After 12601 steps,the loss is 0.512329\n",
      "After 12701 steps,the loss is 0.510963\n",
      "After 12801 steps,the loss is 0.573346\n",
      "After 12901 steps,the loss is 0.514642\n",
      "After 13001 steps,the loss is 0.608858\n",
      "After 13101 steps,the loss is 0.509337\n",
      "After 13201 steps,the loss is 0.800575\n",
      "After 13301 steps,the loss is 0.506083\n",
      "After 13401 steps,the loss is 0.683283\n",
      "After 13501 steps,the loss is 0.503503\n",
      "After 13601 steps,the loss is 0.622734\n",
      "After 13701 steps,the loss is 0.501739\n",
      "After 13801 steps,the loss is 0.502209\n",
      "After 13901 steps,the loss is 0.500681\n",
      "After 14001 steps,the loss is 0.513294\n",
      "After 14101 steps,the loss is 0.499554\n",
      "After 14201 steps,the loss is 0.497046\n",
      "After 14301 steps,the loss is 0.497310\n",
      "After 14401 steps,the loss is 0.499418\n",
      "After 14501 steps,the loss is 1.235166\n",
      "After 14601 steps,the loss is 0.494440\n",
      "After 14701 steps,the loss is 0.495551\n",
      "After 14801 steps,the loss is 0.492160\n",
      "After 14901 steps,the loss is 0.528410\n",
      "After 15001 steps,the loss is 0.489849\n",
      "After 15101 steps,the loss is 0.495805\n",
      "After 15201 steps,the loss is 0.541001\n",
      "After 15301 steps,the loss is 0.490161\n",
      "After 15401 steps,the loss is 0.486616\n",
      "After 15501 steps,the loss is 0.485845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 15601 steps,the loss is 0.484530\n",
      "After 15701 steps,the loss is 0.483706\n",
      "After 15801 steps,the loss is 0.484086\n",
      "After 15901 steps,the loss is 0.481948\n",
      "After 16001 steps,the loss is 0.481106\n",
      "After 16101 steps,the loss is 0.486761\n",
      "After 16201 steps,the loss is 0.481790\n",
      "After 16301 steps,the loss is 0.478480\n",
      "After 16401 steps,the loss is 0.478640\n",
      "After 16501 steps,the loss is 0.476802\n",
      "After 16601 steps,the loss is 0.475985\n",
      "After 16701 steps,the loss is 0.475035\n",
      "After 16801 steps,the loss is 0.714575\n",
      "After 16901 steps,the loss is 0.474666\n",
      "After 17001 steps,the loss is 0.472400\n",
      "After 17101 steps,the loss is 0.471576\n",
      "After 17201 steps,the loss is 0.471715\n",
      "After 17301 steps,the loss is 0.469967\n",
      "After 17401 steps,the loss is 0.469257\n",
      "After 17501 steps,the loss is 0.468196\n",
      "After 17601 steps,the loss is 0.467872\n",
      "After 17701 steps,the loss is 0.466692\n",
      "After 17801 steps,the loss is 0.472051\n",
      "After 17901 steps,the loss is 0.540070\n",
      "After 18001 steps,the loss is 0.463912\n",
      "After 18101 steps,the loss is 0.463043\n",
      "After 18201 steps,the loss is 0.462325\n",
      "After 18301 steps,the loss is 0.470149\n",
      "After 18401 steps,the loss is 0.461545\n",
      "After 18501 steps,the loss is 0.459744\n",
      "After 18601 steps,the loss is 0.458973\n",
      "After 18701 steps,the loss is 0.473234\n",
      "After 18801 steps,the loss is 0.457377\n",
      "After 18901 steps,the loss is 0.456445\n",
      "After 19001 steps,the loss is 0.456711\n",
      "After 19101 steps,the loss is 0.454847\n",
      "After 19201 steps,the loss is 0.454143\n",
      "After 19301 steps,the loss is 0.805508\n",
      "After 19401 steps,the loss is 0.453425\n",
      "After 19501 steps,the loss is 0.457488\n",
      "After 19601 steps,the loss is 0.630311\n",
      "After 19701 steps,the loss is 0.451499\n",
      "After 19801 steps,the loss is 0.648680\n",
      "After 19901 steps,the loss is 0.448252\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import  input_data\n",
    "import mnist_lenet_forward\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "REGULARIZER = 0.0001\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"./lenet_model/\"\n",
    "MODEL_NAME = 'lenet'\n",
    "STEPS = 20000\n",
    "\n",
    "\n",
    "def backward(mnist):\n",
    "    x = tf.placeholder(tf.float32,[\n",
    "        BATCH_SIZE,\n",
    "        mnist_lenet_forward.IMAGE_SIZE,\n",
    "        mnist_lenet_forward.IMAGE_SIZE,\n",
    "        mnist_lenet_forward.NUM_CHANNELS\n",
    "    ])\n",
    "    y_ = tf.placeholder(tf.float32,[None,mnist_lenet_forward.OUTPUT_SIZE])\n",
    "    \n",
    "    y = mnist_lenet_forward.forward(x,True, regularizer=REGULARIZER)\n",
    "    \n",
    "    global_step =  tf.Variable(0,trainable=False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection(\"losses\"))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            mnist.train.num_examples/BATCH_SIZE,\n",
    "            LEARNING_RATE_DECAY\n",
    "    )\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step = global_step)\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op = tf.no_op(name=\"train\")\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            save.restore(sess,ckpt.model_checkpoint_path)\n",
    "            \n",
    "        for i in range(STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshape_xs = np.reshape(xs,(BATCH_SIZE,\n",
    "                                     mnist_lenet_forward.IMAGE_SIZE,\n",
    "                                     mnist_lenet_forward.IMAGE_SIZE,\n",
    "                                     mnist_lenet_forward.NUM_CHANNELS))\n",
    "            _, loss_value, step, = sess.run([train_op,loss,global_step],feed_dict={x:reshape_xs,y_:ys} )\n",
    "            if i % 100 == 0:\n",
    "                print(\"After %d steps,the loss is %f\"%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step = global_step)\n",
    "                \n",
    "            \n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"./MNIST_data/\",one_hot=True)\n",
    "    backward(mnist)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "                \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
